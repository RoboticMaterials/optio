{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davis/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from bson.json_util import dumps, loads\n",
    "from bson.objectid import ObjectId\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.ContactDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_time(dt=None, delta=timedelta(minutes=1), to='average'):\n",
    "    \"\"\"\n",
    "    Round a datetime object to a multiple of a timedelta\n",
    "    dt : datetime.datetime object, default now.\n",
    "    dateDelta : timedelta object, we round to a multiple of this, default 1 minute.\n",
    "    from:  http://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object-python\n",
    "    \"\"\"\n",
    "    minDt = datetime.now().replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)  \n",
    "    \n",
    "    round_to = delta.total_seconds()\n",
    "    if dt is None:\n",
    "        dt = datetime.now()\n",
    "    seconds = (dt - minDt).seconds\n",
    "\n",
    "    if seconds % round_to == 0 and dt.microsecond == 0:\n",
    "        rounding = (seconds + round_to / 2) // round_to * round_to\n",
    "    else:\n",
    "        if to == 'ceil':\n",
    "            # // is a floor division, not a comment on following line (like in javascript):\n",
    "            rounding = (seconds + dt.microsecond/1000000 + round_to) // round_to * round_to\n",
    "        elif to == 'floor':\n",
    "            rounding = seconds // round_to * round_to\n",
    "        else:\n",
    "            rounding = (seconds + round_to / 2) // round_to * round_to\n",
    "\n",
    "    return dt + timedelta(0, rounding - seconds, - dt.microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_summary(station_id, summary_period, current_dateTime=None):\n",
    "    ## Generate the next summary of a days events\n",
    "    \n",
    "    if current_dateTime is None:\n",
    "        current_dateTime = datetime.now()\n",
    "        \n",
    "    # Get latest summary dateTime\n",
    "    summary_cursor = list(db.daily_summaries.find({}).sort([(\"_id\",-1)]).limit(1))\n",
    "    if len(summary_cursor) == 0:\n",
    "        # No current summary entries, use first event as latest datetime\n",
    "        all_events = list(db.touch_events.find({}).sort([(\"_id\",-1)]))\n",
    "        latest_summary_dateTime = all_events[-1]['move_datetime']\n",
    "    else:\n",
    "        latest_summary_dateTime = summary_cursor[0]['datetime']\n",
    "        \n",
    "    # floor the most recent datetime to summary perioud\n",
    "    floored_latest_summary_dateTime = round_time(latest_summary_dateTime, delta=summary_period, to='floor')\n",
    "    # Cutoff for next summary is the previous floored summary datetime + the delta\n",
    "    next_summary_dateTime = floored_latest_summary_dateTime + summary_period \n",
    "    \n",
    "        \n",
    "    if current_dateTime > next_summary_dateTime:  # If last summary was more then the summary period ago, make a new summary\n",
    "        \n",
    "        # Convert from datetime back into timestamp\n",
    "        next_summary_dateTime = datetime.timestamp(next_summary_dateTime) * 1000\n",
    "\n",
    "        ## Time to generate new summary\n",
    "        # Gets all events in the 6hr period following the most recent summary\n",
    "        next_touch_events = db.touch_events.find({\n",
    "            '$and': [\n",
    "                {'load_station_id': station_id},\n",
    "                {'move_datetime': {'$gt': latest_summary_dateTime}},\n",
    "                {'move_datetime': {'$lte': next_summary_dateTime}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        next_report_events = db.report_events.find({\n",
    "            '$and': [\n",
    "                {'load_station_id': station_id},\n",
    "                {'datetime': {'$gt': latest_summary_dateTime}},\n",
    "                {'datetime': {'$lte': next_summary_dateTime}}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        return summarize_events(next_touch_events, next_report_events, next_summary_dateTime)\n",
    "\n",
    "    else:   # Otherwise there is no new summary, return None\n",
    "        print('nothing to summarize')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_events(touch_events, report_events, summary_date):\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_quantities_to_line_data(df, bins):\n",
    "    # Pass this a df and bins and it will output line data with the x being the bin and y being the summation of quantity for events within the bin window\n",
    "    cut_data = pd.cut(df['move_datetime'], bins)\n",
    "    grouped_data = list(df.groupby(cut_data)['quantity'].sum())\n",
    "    \n",
    "    return list(map(lambda x, y: {'x': x, 'y': y}, bins, grouped_data))\n",
    "\n",
    "def bin_wip_to_lines_data(df, bins, product_groups):\n",
    "    cut_data = pd.cut(df['move_datetime'], bins)\n",
    "    grouped_data = list(df.groupby(cut_data)['current_wip'])\n",
    "\n",
    "    wip_data_dict = {}\n",
    "    for pg in product_groups:\n",
    "        wip_data_dict[pg['_id']] = {\n",
    "            'id': pg['name'],\n",
    "            'data': []\n",
    "        }\n",
    "        \n",
    "    for bin_data in grouped_data:\n",
    "        \n",
    "        values_df = pd.DataFrame(bin_data[1].values.tolist())\n",
    "        max_wip = {key: values_df[key].max() for key in values_df.columns}\n",
    "        \n",
    "        for pg in product_groups:\n",
    "            if pg['_id'] in max_wip:\n",
    "                wip_data_dict[pg['_id']]['data'].append({'x': bin_data[0].right, 'y': max_wip[pg['_id']]})\n",
    "            else:\n",
    "                wip_data_dict[pg['_id']]['data'].append({'x': bin_data[0].right, 'y': 0})\n",
    "            \n",
    "    return wip_data_dict.values()\n",
    "\n",
    "def product_group_pie(df, product_groups):\n",
    "    grouped_df = list(df.groupby('product_group_id'))\n",
    "    \n",
    "    pg_pie = []\n",
    "    for pg_id, pg_df in grouped_df:\n",
    "        pg_pie.append({\n",
    "            'id': pg_id,\n",
    "            'label': product_groups[pg_id]['name'],\n",
    "            'value': pg_df['quantity'].sum()\n",
    "        })\n",
    "        \n",
    "    return pg_pie\n",
    "\n",
    "def bin_reports_to_bar_data(df, bins, colors):\n",
    "    # Bin data\n",
    "    cut_data = pd.cut(df['datetime'], bins)\n",
    "    binned_data = list(df.groupby(cut_data))\n",
    "\n",
    "    bar_data = []\n",
    "    for data in binned_data:\n",
    "        bar = {\n",
    "            \"x\": data[0].right # X value is the timestep to the right of the bin\n",
    "        }\n",
    "        # Organize the reports by name, and set the bar to the count of instances of that report\n",
    "        value_counts = data[1]['report_name'].value_counts().to_dict()\n",
    "        for key, value in value_counts.items():\n",
    "            bar[key] = value\n",
    "        \n",
    "        # Populate the bar with the colors that we mapped earlier\n",
    "        for report_name, color in colors.items():\n",
    "            bar[report_name + 'Color'] = color\n",
    "            \n",
    "        bar_data.append(bar)\n",
    "\n",
    "    return bar_data\n",
    "\n",
    "def reports_pie(df, colors):\n",
    "    report_counts = df['report_name'].value_counts().to_dict()\n",
    "    \n",
    "    report_pie = []\n",
    "    for report_name, count in report_counts.items():\n",
    "        report_pie.append({\n",
    "            \"id\": report_name,\n",
    "            \"label\": report_name, \n",
    "            \"value\": count,\n",
    "            \"color\": colors[report_name]\n",
    "        })\n",
    "        \n",
    "    return report_pie\n",
    "\n",
    "def generate_statistics_from_events(station_id, touch_events, report_events):\n",
    "    touches_df = pd.DataFrame(touch_events)    \n",
    "    te_count = len(touch_events)\n",
    "        \n",
    "    station = db.stations.find_one({'_id': station_id})\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    if te_count == 0:\n",
    "        product_group_ids = []\n",
    "    else:\n",
    "        product_group_ids = touches_df['product_group_id'].unique()\n",
    "    product_groups = {}\n",
    "    for pg_id in product_group_ids:\n",
    "        product_groups[pg_id] = db.lot_templates.find_one({'_id': pg_id})\n",
    "\n",
    "    # TODO: Implement this and delete from this function\n",
    "    station['desired_cycle_times'] = {}\n",
    "    station['theo_min_cycle_times'] = {}\n",
    "    for pg_id in product_group_ids:\n",
    "        station['desired_cycle_times'][pg_id] = 1\n",
    "        station['theo_min_cycle_times'][pg_id] = 0.8\n",
    "    # ===================================================\n",
    "    \n",
    "    stats['efficiency'] = []\n",
    "    stats['oee'] = []\n",
    "    stats['cycle_time'] = {}\n",
    "    \n",
    "    found_pg_ids = []\n",
    "    for touch_event in touch_events[::-1]: # Reverse order because we want to take the stats at the end of the day\n",
    "        pg_id = touch_event['product_group_id']\n",
    "                \n",
    "        # First time weve found this product group, this is the latest data we have\n",
    "        if pg_id not in found_pg_ids:\n",
    "            found_pg_ids.append(pg_id)\n",
    "            \n",
    "            # --- Efficieny\n",
    "            stats['efficiency'].append({\n",
    "                'id': product_groups[pg_id]['name'],\n",
    "                'data': [{\n",
    "                    'x': 'Efficiency',\n",
    "                    'y': station['desired_cycle_times'][pg_id] / touch_event['pgs_cycle_time']\n",
    "                }]\n",
    "            })\n",
    "            \n",
    "            # --- OEE\n",
    "            stats['oee'].append({\n",
    "                'id': product_groups[pg_id]['name'],\n",
    "                'data': [{\n",
    "                    'x': 'OEE',\n",
    "                    'y': station['theo_min_cycle_times'][pg_id] / touch_event['pgs_cycle_time']\n",
    "                }]\n",
    "            })\n",
    "            \n",
    "            # --- Cycle Time\n",
    "            stats['cycle_time'][pg_id] = {\n",
    "                'current': touch_event['pgs_cycle_time'],\n",
    "                'line_data': [{\n",
    "                    'id': product_groups[pg_id]['name'],\n",
    "                    'data': [{\n",
    "                        'x': touch_event['move_datetime'],\n",
    "                        'y': touch_event['pgs_cycle_time']\n",
    "                    }]\n",
    "                }]\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # --- Cycle Time\n",
    "            # There is already data for this product group, insert it at beginnning (since we are going through events in reverse order)\n",
    "            stats['cycle_time'][pg_id]['line_data'][0]['data'].insert(0, {\n",
    "                'x': touch_event['move_datetime'],\n",
    "                'y': touch_event['pgs_cycle_time']\n",
    "            })\n",
    "            \n",
    "    # === Generate bins\n",
    "    if te_count == 0:\n",
    "        bins = []\n",
    "    else:\n",
    "        start_hour = round_time(touch_events[0]['move_datetime'], timedelta(hours=1), 'floor')\n",
    "        end_hour = round_time(touch_events[-1]['move_datetime'], timedelta(hours=1), 'ceil')\n",
    "        print(start_hour, end_hour)\n",
    "        delta_hours = (end_hour - start_hour).seconds // 3600\n",
    "        \n",
    "        bins = [start_hour]\n",
    "        for delta_hours in range(delta_hours):\n",
    "            bins.append(start_hour + timedelta(hours=delta_hours+1))\n",
    "        \n",
    "\n",
    "    # === LINE GRAPHS\n",
    "    stats['throughput'] = []\n",
    "    stats['wip'] = []\n",
    "    stats['product_group_pie'] = []\n",
    "    stats['reports'] = []\n",
    "    stats['reports_pie'] = []\n",
    "    \n",
    "    if te_count > 0:\n",
    "        # --- Throughput\n",
    "        stats['throughput'].append({\n",
    "            'id': 'Total',\n",
    "            'data': bin_quantities_to_line_data(touches_df, bins)\n",
    "        })\n",
    "        for pg_id in product_group_ids:\n",
    "            stats['throughput'].append({\n",
    "                'id': product_groups[pg_id]['name'],\n",
    "                'data': bin_quantities_to_line_data(touches_df[touches_df['product_group_id'] == pg_id], bins)\n",
    "            })\n",
    "            \n",
    "        # --- WIP\n",
    "        stats['wip'] = bin_wip_to_lines_data(touches_df, bins, product_groups.values())\n",
    "        \n",
    "        # --- Product Group Pie\n",
    "        stats['product_group_pie'] = product_group_pie(touches_df, product_groups)\n",
    "        \n",
    "    if len(report_events) > 0:\n",
    "        # === Reports\n",
    "        # Instead of querying the dashboards every time, fill in the dataframe with report name and report color\n",
    "        dashboard = db.dashboards.find_one({'station': station_id})\n",
    "        report_buttons_normalized = {x['_id']: x for x in dashboard['report_buttons']}\n",
    "\n",
    "        reports_colors = {} # Also store a dictionary of all colors, with the key being the report name\n",
    "        for i, re in enumerate(report_events):\n",
    "            rb_id = re['report_button_id']\n",
    "            if rb_id in report_buttons_normalized:\n",
    "                report_name = report_buttons_normalized[rb_id]['label']\n",
    "                report_events[i]['report_name'] = report_name\n",
    "                reports_colors[report_name] = report_buttons_normalized[rb_id]['color']\n",
    "\n",
    "        # Convert to dataframe\n",
    "        reports_df = pd.DataFrame(report_events)\n",
    "\n",
    "        # --- Reports Bar\n",
    "        stats['reports'] = bin_reports_to_bar_data(reports_df, bins, reports_colors)\n",
    "        \n",
    "        # --- Reports Pie\n",
    "        stats['reports_pie'] = reports_pie(reports_df, reports_colors)\n",
    "            \n",
    "    return stats['product_group_pie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-05 00:00:00\n",
      "2021-11-05 13:00:00 2021-11-05 14:00:00\n",
      "[{'id': '6166150652325faeb6d12b7d', 'label': 'Snowboard', 'value': 10},\n",
      " {'id': '6172ebb010e2161e81d01f2e', 'label': 'Basic', 'value': 10},\n",
      " {'id': '6180250995773c687755558b', 'label': 'Davis', 'value': 45}]\n"
     ]
    }
   ],
   "source": [
    "station_id = '35dc9b69-3d93-4a83-8a25-99432edc43b0'\n",
    "day_start = round_time(datetime.today(), timedelta(days=1), 'floor')\n",
    "print(day_start)\n",
    "\n",
    "tes = list(db.touch_events.find({'$and': [{'load_station_id': station_id}, {'move_datetime': {'$gte': day_start}}]}).sort([(\"move_datetime\",1)]))\n",
    "res = list(db.report_events.find({'$and': [{'station_id': station_id}, {'datetime': {'$gte': day_start}}]}).sort([(\"move_datetime\",1)]))\n",
    "\n",
    "# TODO: Implement this and remove from here\n",
    "for i, te in enumerate(tes):\n",
    "    tes[i]['pgs_cycle_time'] = calculate_pg_station_cycle_time(te['load_station_id'], te['product_group_id'])\n",
    "    if tes[i]['pgs_cycle_time'] == 0:\n",
    "        tes[i]['pgs_cycle_time'] = 10\n",
    "\n",
    "pprint(generate_statistics_from_events(station_id, tes, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "\n",
    "# == = = = = = = = = == = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics_from_summaries(summaries):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'efficiency': {'[product_group_id]': 100},\n",
       " 'overall_efficiency': 100,\n",
       " 'oee': {'[product_group_id]': 100},\n",
       " 'overall_oee': 100,\n",
       " 'cycle_time': {'[product_group_id]': {'line_data': [{'x': None, 'y': None}],\n",
       "   'current_cycle_time': '00:00:00'}},\n",
       " 'throughput': {'_total': [{'x': None}, {'y': None}],\n",
       "  '[product_group_id]': [{'x': None}, {'y': None}]},\n",
       " 'wip': [{'x': None, '[product_group_name]': None}],\n",
       " 'station_reports': [{'x': None, '[product_group_name]': None}],\n",
       " 'station_reports_pie': {'[report_name]': 100},\n",
       " 'product_group_pie': {'[product_group_name]': 100},\n",
       " 'operator_pie': {'[operator]': 100},\n",
       " 'output_station': {'[station_name]': 100},\n",
       " 'machine_utilization': [{'Working': 100, 'WorkingColor': 'green'},\n",
       "  {'Idle': 100, 'IdleColor': 'green'}],\n",
       " 'value_creating_time': [{'Working': 100, 'WorkingColor': 'green'},\n",
       "  {'Idle': 100, 'IdleColor': 'green'}],\n",
       " 'touch_events': [],\n",
       " 'report_events': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    # Efficiency\n",
    "    'efficiency': {\n",
    "        '[product_group_id]': 100\n",
    "    },\n",
    "    'overall_efficiency': 100,\n",
    "    \n",
    "    # OEE\n",
    "    'oee': {\n",
    "        '[product_group_id]': 100\n",
    "    },\n",
    "    'overall_oee': 100,\n",
    "    \n",
    "    # Cycle Time\n",
    "    'cycle_time': {\n",
    "        '[product_group_id]': {\n",
    "            'line_data': [{'x': None, 'y': None}],\n",
    "            'current_cycle_time': \"00:00:00\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Throughput\n",
    "    'throughput': {\n",
    "        '_total': [\n",
    "            {'x': None},\n",
    "            {'y': None}\n",
    "        ],\n",
    "        '[product_group_id]' : [\n",
    "            {'x': None},\n",
    "            {'y': None}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # WIP (stacked histogram)\n",
    "    'wip': [\n",
    "        {\n",
    "            'x': None,\n",
    "            '[product_group_name]': None\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # Station Reports\n",
    "    'station_reports': [\n",
    "        {\n",
    "            'x': None,\n",
    "            '[product_group_name]': None\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # Pie Charts\n",
    "    'station_reports_pie': {\n",
    "        '[report_name]': 100\n",
    "    },\n",
    "    'product_group_pie': {\n",
    "        '[product_group_name]': 100\n",
    "    },\n",
    "    'operator_pie': {\n",
    "        '[operator]': 100\n",
    "    },\n",
    "    'output_station': {\n",
    "        '[station_name]': 100\n",
    "    },\n",
    "    \n",
    "    # Machine Utilization\n",
    "    'machine_utilization': [\n",
    "        {\n",
    "            'Working': 100,\n",
    "            'WorkingColor': 'green'\n",
    "        },\n",
    "        {\n",
    "            'Idle': 100,\n",
    "            'IdleColor': 'green'\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # Value Creating Time\n",
    "    'value_creating_time': [\n",
    "        {\n",
    "            'Working': 100,\n",
    "            'WorkingColor': 'green'\n",
    "        },\n",
    "        {\n",
    "            'Idle': 100,\n",
    "            'IdleColor': 'green'\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # NOTE: This needs to pe paginated somehow\n",
    "    'touch_events': [],\n",
    "    'report_events': []\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_daily_working_seconds(shift_details, start_time, stop_time):\n",
    "    \n",
    "    begin_shift = time.strptime(shift_details['startOfShift'],'%H:%M')\n",
    "    begin_shift_secs = timedelta(hours=begin_shift.tm_hour,minutes=begin_shift.tm_min,seconds=begin_shift.tm_sec).total_seconds()\n",
    "\n",
    "    # Cant start before the shift starts\n",
    "    if start_time is not None:\n",
    "        start_time_secs = timedelta(hours=start_time.hour,minutes=start_time.minute,seconds=start_time.second).total_seconds()\n",
    "        start_time_secs = max(start_time_secs, begin_shift_secs)\n",
    "    else: \n",
    "        start_time_secs = begin_shift_secs\n",
    "    \n",
    "    end_of_shift = time.strptime(shift_details['endOfShift'],'%H:%M')\n",
    "    end_of_shift_secs = timedelta(hours=end_of_shift.tm_hour,minutes=end_of_shift.tm_min,seconds=end_of_shift.tm_sec).total_seconds()\n",
    "    \n",
    "    if stop_time is not None:\n",
    "        stop_time_secs = timedelta(hours=stop_time.hour,minutes=stop_time.minute,seconds=stop_time.second).total_seconds()\n",
    "        end_time_secs = min(stop_time_secs, end_of_shift_secs)\n",
    "    else:\n",
    "        end_time_secs = end_of_shift_secs\n",
    "    \n",
    "    # Total working seconds between the start and stop not including breaks\n",
    "    working_seconds = max(0, (end_time_secs - start_time_secs))\n",
    "    \n",
    "    # Need to calculate the working time using shift details \n",
    "    breaks = shift_details['breaks'].values()\n",
    "    # For each break, calculate the length of the break in seconds and ad to total break time\n",
    "    for br in breaks:\n",
    "        if(br['enabled']):\n",
    "            br_start_time = time.strptime(br['startOfBreak'],'%H:%M')\n",
    "            br_start_secs = timedelta(hours=br_start_time.tm_hour,minutes=br_start_time.tm_min,seconds=br_start_time.tm_sec).total_seconds()\n",
    "            br_start_secs = min(start_time_secs, max(br_start_secs, stop_time_secs))\n",
    "            \n",
    "            br_end_time = time.strptime(br['endOfBreak'],'%H:%M')\n",
    "            br_end_secs = timedelta(hours=br_end_time.tm_hour,minutes=br_end_time.tm_min,seconds=br_end_time.tm_sec).total_seconds()\n",
    "            br_end_secs = max(start_time_secs, min(br_end_secs, stop_time_secs))\n",
    "            \n",
    "            working_seconds -= (br_end_secs - br_start_secs)\n",
    "            \n",
    "    return working_seconds\n",
    "    \n",
    "\n",
    "def calculate_pg_station_cycle_time(station_id, product_group_id, new_event=None):\n",
    "    events = list(db.touch_events.find({'$and': [{'load_station_id':station_id}, {'product_group_id': product_group_id}]}).sort('move_datetime').limit(30))\n",
    "    if new_event is not None:\n",
    "        events.append(new_event)\n",
    "    settings = db.settings.find_one()\n",
    "    \n",
    "    # Fetch data\n",
    "    df = pd.DataFrame(events)\n",
    "    \n",
    "    if len(df) > 1:\n",
    "    \n",
    "        \n",
    "        shift_details = settings['shiftDetails']\n",
    "        begin_shift = time.strptime(shift_details['startOfShift'],'%H:%M')\n",
    "        begin_shift_secs = timedelta(hours=begin_shift.tm_hour,minutes=begin_shift.tm_min,seconds=begin_shift.tm_sec).total_seconds()\n",
    "        end_of_shift = time.strptime(shift_details['endOfShift'],'%H:%M')\n",
    "        end_of_shift_secs = timedelta(hours=end_of_shift.tm_hour,minutes=end_of_shift.tm_min,seconds=end_of_shift.tm_sec).total_seconds()\n",
    "        \n",
    "        # Total quantity of parts moved\n",
    "        total_quantity = 0\n",
    "        for i, event in df.iterrows():\n",
    "            # Only count moves that were within the shift\n",
    "            event_stop_time = event['move_datetime']\n",
    "            event_stop_time_secs = timedelta(hours=event_stop_time.hour,minutes=event_stop_time.minute,seconds=event_stop_time.second).total_seconds()\n",
    "            if event_stop_time_secs > begin_shift_secs and event_stop_time_secs < end_of_shift_secs:\n",
    "                total_quantity += event['quantity']\n",
    "                                \n",
    "        # If quantity is 0, nothing we can do\n",
    "        if total_quantity == 0:\n",
    "            return 0\n",
    "        \n",
    "        first_event_time = df.iloc[0]['move_datetime']\n",
    "        last_event_time = df.iloc[-1]['move_datetime']\n",
    "                \n",
    "        \n",
    "        if first_event_time.date() == last_event_time.date():\n",
    "            total_working_seconds = calc_daily_working_seconds(shift_details, first_event_time.time(), last_event_time.time())\n",
    "        else:\n",
    "            first_day_working_seconds = calc_daily_working_seconds(shift_details, first_event_time.time(), None)\n",
    "            last_day_working_seconds = calc_daily_working_seconds(shift_details, None, last_event_time.time())\n",
    "            \n",
    "            daily_working_seconds = calc_daily_working_seconds(shift_details, None, None)\n",
    "            \n",
    "            # Find unique dates to be used later\n",
    "            unique_dates = []\n",
    "            for datetime in df['move_datetime']:\n",
    "                rounded_date = datetime.date()\n",
    "                if rounded_date not in unique_dates: unique_dates.append(rounded_date)\n",
    "                \n",
    "            total_working_seconds = first_day_working_seconds + daily_working_seconds*(len(unique_dates)-2) + last_day_working_seconds\n",
    "            \n",
    "        cycle_time = total_working_seconds / total_quantity\n",
    "        return cycle_time   \n",
    "                     \n",
    "    else:\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(station_id, current_dateTime=None):\n",
    "     # summarize all 10min summaries to catch up to current date\n",
    "    summary = generate_next_summary(station_id, timedelta(days=1), current_dateTime)\n",
    "    while summary is not None:\n",
    "        # db.historicals_day.insert_one(summary)\n",
    "        summary = generate_next_summary(station_id, timedelta(days=1), current_dateTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb139d972f27678f5902068e8469cd00f9fe0b75851b9da3573e0e77756ebf38"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
